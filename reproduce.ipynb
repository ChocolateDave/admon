{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display\n",
    "from typing import Union\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchinfo\n",
    "from sklearn.metrics import normalized_mutual_info_score, f1_score\n",
    "\n",
    "from admon.model import DMoN\n",
    "from admon.utils import load_npz, modularity, conductance\n",
    "\n",
    "# Alias\n",
    "_PathLike = Union[str, 'os.PathLike[str]']\n",
    "CORA_DIR: _PathLike = './data/cora'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features, labels, label_indices = load_npz(os.path.join(CORA_DIR, 'cora.npz'))\n",
    "adj_tensor = T.tensor(adj.todense()).unsqueeze(0).float()\n",
    "features_tensor = T.tensor(features.todense()).unsqueeze(0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juanwu/Documents/spring_22/CE290_2/project/admon/admon/model/model.py:65: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
      "  self.predict.weight.copy_(nn.init.orthogonal(self.predict.weight.data))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "DMoN                                     --\n",
       "├─Single: 1-1                            --\n",
       "│    └─Sequential: 2-1                   --\n",
       "│    │    └─GCN: 3-1                     91,712\n",
       "│    └─Linear: 2-2                       1,040\n",
       "│    └─Dropout: 2-3                      --\n",
       "=================================================================\n",
       "Total params: 92,752\n",
       "Trainable params: 92,752\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameter\n",
    "n_clusters: int = 16\n",
    "num_epochs: int = 200\n",
    "hidden: int = 64\n",
    "depths: int = 1\n",
    "dropout: float = 0.\n",
    "inflation: int = 1\n",
    "activation: str= 'selu'\n",
    "collapse_regularization = .01\n",
    "device = T.device('cpu')\n",
    "\n",
    "lr: float = 1e-3\n",
    "weight_decay: float = 5e-4\n",
    "lr_decay_step: int = 5\n",
    "lr_decay_gamma: float = 0.3\n",
    "\n",
    "model = DMoN(features_tensor.size(-1), n_clusters,\n",
    "             hidden, depths, dropout, inflation,\n",
    "             collapse_regularization=collapse_regularization)\n",
    "model: nn.Module = model.to(device)\n",
    "torchinfo.summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] Loss: 27.06\n",
      "Epoch [2/200] Loss: 27.06\n",
      "Epoch [3/200] Loss: 27.06\n",
      "Epoch [4/200] Loss: 27.06\n",
      "Epoch [5/200] Loss: 27.06\n",
      "Epoch [6/200] Loss: 27.06\n",
      "Epoch [7/200] Loss: 27.06\n",
      "Epoch [8/200] Loss: 27.06\n",
      "Epoch [9/200] Loss: 27.06\n",
      "Epoch [10/200] Loss: 27.05\n",
      "Epoch [11/200] Loss: 27.05\n",
      "Epoch [12/200] Loss: 27.05\n",
      "Epoch [13/200] Loss: 27.05\n",
      "Epoch [14/200] Loss: 27.05\n",
      "Epoch [15/200] Loss: 27.05\n",
      "Epoch [16/200] Loss: 27.05\n",
      "Epoch [17/200] Loss: 27.05\n",
      "Epoch [18/200] Loss: 27.05\n",
      "Epoch [19/200] Loss: 27.05\n",
      "Epoch [20/200] Loss: 27.05\n",
      "Epoch [21/200] Loss: 27.05\n",
      "Epoch [22/200] Loss: 27.05\n",
      "Epoch [23/200] Loss: 27.05\n",
      "Epoch [24/200] Loss: 27.05\n",
      "Epoch [25/200] Loss: 27.05\n",
      "Epoch [26/200] Loss: 27.05\n",
      "Epoch [27/200] Loss: 27.05\n",
      "Epoch [28/200] Loss: 27.05\n",
      "Epoch [29/200] Loss: 27.05\n",
      "Epoch [30/200] Loss: 27.05\n",
      "Epoch [31/200] Loss: 27.05\n",
      "Epoch [32/200] Loss: 27.05\n",
      "Epoch [33/200] Loss: 27.05\n",
      "Epoch [34/200] Loss: 27.05\n",
      "Epoch [35/200] Loss: 27.05\n",
      "Epoch [36/200] Loss: 27.05\n",
      "Epoch [37/200] Loss: 27.05\n",
      "Epoch [38/200] Loss: 27.05\n",
      "Epoch [39/200] Loss: 27.05\n",
      "Epoch [40/200] Loss: 27.05\n",
      "Epoch [41/200] Loss: 27.05\n",
      "Epoch [42/200] Loss: 27.05\n",
      "Epoch [43/200] Loss: 27.05\n",
      "Epoch [44/200] Loss: 27.05\n",
      "Epoch [45/200] Loss: 27.05\n",
      "Epoch [46/200] Loss: 27.05\n",
      "Epoch [47/200] Loss: 27.05\n",
      "Epoch [48/200] Loss: 27.05\n",
      "Epoch [49/200] Loss: 27.05\n",
      "Epoch [50/200] Loss: 27.05\n",
      "Epoch [51/200] Loss: 27.05\n",
      "Epoch [52/200] Loss: 27.05\n",
      "Epoch [53/200] Loss: 27.05\n",
      "Epoch [54/200] Loss: 27.05\n",
      "Epoch [55/200] Loss: 27.05\n",
      "Epoch [56/200] Loss: 27.05\n",
      "Epoch [57/200] Loss: 27.05\n",
      "Epoch [58/200] Loss: 27.05\n",
      "Epoch [59/200] Loss: 27.05\n",
      "Epoch [60/200] Loss: 27.05\n",
      "Epoch [61/200] Loss: 27.05\n",
      "Epoch [62/200] Loss: 27.05\n",
      "Epoch [63/200] Loss: 27.05\n",
      "Epoch [64/200] Loss: 27.05\n",
      "Epoch [65/200] Loss: 27.05\n",
      "Epoch [66/200] Loss: 27.05\n",
      "Epoch [67/200] Loss: 27.05\n",
      "Epoch [68/200] Loss: 27.05\n",
      "Epoch [69/200] Loss: 27.05\n",
      "Epoch [70/200] Loss: 27.05\n",
      "Epoch [71/200] Loss: 27.05\n",
      "Epoch [72/200] Loss: 27.05\n",
      "Epoch [73/200] Loss: 27.05\n",
      "Epoch [74/200] Loss: 27.05\n",
      "Epoch [75/200] Loss: 27.05\n",
      "Epoch [76/200] Loss: 27.05\n",
      "Epoch [77/200] Loss: 27.05\n",
      "Epoch [78/200] Loss: 27.05\n",
      "Epoch [79/200] Loss: 27.05\n",
      "Epoch [80/200] Loss: 27.05\n",
      "Epoch [81/200] Loss: 27.05\n",
      "Epoch [82/200] Loss: 27.05\n",
      "Epoch [83/200] Loss: 27.05\n",
      "Epoch [84/200] Loss: 27.05\n",
      "Epoch [85/200] Loss: 27.05\n",
      "Epoch [86/200] Loss: 27.05\n",
      "Epoch [87/200] Loss: 27.05\n",
      "Epoch [88/200] Loss: 27.05\n",
      "Epoch [89/200] Loss: 27.05\n",
      "Epoch [90/200] Loss: 27.05\n",
      "Epoch [91/200] Loss: 27.05\n",
      "Epoch [92/200] Loss: 27.05\n",
      "Epoch [93/200] Loss: 27.05\n",
      "Epoch [94/200] Loss: 27.05\n",
      "Epoch [95/200] Loss: 27.05\n",
      "Epoch [96/200] Loss: 27.05\n",
      "Epoch [97/200] Loss: 27.05\n",
      "Epoch [98/200] Loss: 27.05\n",
      "Epoch [99/200] Loss: 27.05\n",
      "Epoch [100/200] Loss: 27.05\n",
      "Epoch [101/200] Loss: 27.05\n",
      "Epoch [102/200] Loss: 27.05\n",
      "Epoch [103/200] Loss: 27.05\n",
      "Epoch [104/200] Loss: 27.05\n",
      "Epoch [105/200] Loss: 27.05\n",
      "Epoch [106/200] Loss: 27.05\n",
      "Epoch [107/200] Loss: 27.05\n",
      "Epoch [108/200] Loss: 27.05\n",
      "Epoch [109/200] Loss: 27.05\n",
      "Epoch [110/200] Loss: 27.05\n",
      "Epoch [111/200] Loss: 27.05\n",
      "Epoch [112/200] Loss: 27.05\n",
      "Epoch [113/200] Loss: 27.05\n",
      "Epoch [114/200] Loss: 27.05\n",
      "Epoch [115/200] Loss: 27.05\n",
      "Epoch [116/200] Loss: 27.05\n",
      "Epoch [117/200] Loss: 27.05\n",
      "Epoch [118/200] Loss: 27.05\n",
      "Epoch [119/200] Loss: 27.05\n",
      "Epoch [120/200] Loss: 27.05\n",
      "Epoch [121/200] Loss: 27.05\n",
      "Epoch [122/200] Loss: 27.05\n",
      "Epoch [123/200] Loss: 27.05\n",
      "Epoch [124/200] Loss: 27.05\n",
      "Epoch [125/200] Loss: 27.05\n",
      "Epoch [126/200] Loss: 27.05\n",
      "Epoch [127/200] Loss: 27.05\n",
      "Epoch [128/200] Loss: 27.05\n",
      "Epoch [129/200] Loss: 27.05\n",
      "Epoch [130/200] Loss: 27.05\n",
      "Epoch [131/200] Loss: 27.05\n",
      "Epoch [132/200] Loss: 27.05\n",
      "Epoch [133/200] Loss: 27.05\n",
      "Epoch [134/200] Loss: 27.05\n",
      "Epoch [135/200] Loss: 27.05\n",
      "Epoch [136/200] Loss: 27.05\n",
      "Epoch [137/200] Loss: 27.05\n",
      "Epoch [138/200] Loss: 27.05\n",
      "Epoch [139/200] Loss: 27.05\n",
      "Epoch [140/200] Loss: 27.05\n",
      "Epoch [141/200] Loss: 27.05\n",
      "Epoch [142/200] Loss: 27.05\n",
      "Epoch [143/200] Loss: 27.05\n",
      "Epoch [144/200] Loss: 27.05\n",
      "Epoch [145/200] Loss: 27.05\n",
      "Epoch [146/200] Loss: 27.05\n",
      "Epoch [147/200] Loss: 27.05\n",
      "Epoch [148/200] Loss: 27.05\n",
      "Epoch [149/200] Loss: 27.05\n",
      "Epoch [150/200] Loss: 27.05\n",
      "Epoch [151/200] Loss: 27.05\n",
      "Epoch [152/200] Loss: 27.05\n",
      "Epoch [153/200] Loss: 27.05\n",
      "Epoch [154/200] Loss: 27.05\n",
      "Epoch [155/200] Loss: 27.05\n",
      "Epoch [156/200] Loss: 27.05\n",
      "Epoch [157/200] Loss: 27.05\n",
      "Epoch [158/200] Loss: 27.05\n",
      "Epoch [159/200] Loss: 27.05\n",
      "Epoch [160/200] Loss: 27.05\n",
      "Epoch [161/200] Loss: 27.05\n",
      "Epoch [162/200] Loss: 27.05\n",
      "Epoch [163/200] Loss: 27.05\n",
      "Epoch [164/200] Loss: 27.05\n",
      "Epoch [165/200] Loss: 27.05\n",
      "Epoch [166/200] Loss: 27.05\n",
      "Epoch [167/200] Loss: 27.05\n",
      "Epoch [168/200] Loss: 27.05\n",
      "Epoch [169/200] Loss: 27.05\n",
      "Epoch [170/200] Loss: 27.05\n",
      "Epoch [171/200] Loss: 27.05\n",
      "Epoch [172/200] Loss: 27.05\n",
      "Epoch [173/200] Loss: 27.05\n",
      "Epoch [174/200] Loss: 27.05\n",
      "Epoch [175/200] Loss: 27.05\n",
      "Epoch [176/200] Loss: 27.05\n",
      "Epoch [177/200] Loss: 27.05\n",
      "Epoch [178/200] Loss: 27.05\n",
      "Epoch [179/200] Loss: 27.05\n",
      "Epoch [180/200] Loss: 27.05\n",
      "Epoch [181/200] Loss: 27.05\n",
      "Epoch [182/200] Loss: 27.05\n",
      "Epoch [183/200] Loss: 27.05\n",
      "Epoch [184/200] Loss: 27.05\n",
      "Epoch [185/200] Loss: 27.05\n",
      "Epoch [186/200] Loss: 27.05\n",
      "Epoch [187/200] Loss: 27.05\n",
      "Epoch [188/200] Loss: 27.05\n",
      "Epoch [189/200] Loss: 27.05\n",
      "Epoch [190/200] Loss: 27.05\n",
      "Epoch [191/200] Loss: 27.05\n",
      "Epoch [192/200] Loss: 27.05\n",
      "Epoch [193/200] Loss: 27.05\n",
      "Epoch [194/200] Loss: 27.05\n",
      "Epoch [195/200] Loss: 27.05\n",
      "Epoch [196/200] Loss: 27.05\n",
      "Epoch [197/200] Loss: 27.05\n",
      "Epoch [198/200] Loss: 27.05\n",
      "Epoch [199/200] Loss: 27.05\n",
      "Epoch [200/200] Loss: 27.05\n"
     ]
    }
   ],
   "source": [
    "# Train main\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, lr_decay_step, lr_decay_gamma)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  pooled_features, assignments, m_loss, c_loss = model((features_tensor, adj_tensor))\n",
    "  loss: T.Tensor = m_loss + c_loss\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  lr_scheduler.step()\n",
    "\n",
    "  print(f'Epoch [{epoch+1:d}/{num_epochs:d}] Loss: {loss.item():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.038974025608526655"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster_labels = assignments.detach().cpu().numpy().argmax(axis=-1)\n",
    "display(modularity(adj, cluster_labels), conductance(adj, cluster_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1640,), (1, 2708))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape, cluster_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.060144042121365406"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(normalized_mutual_info_score(labels, cluster_labels[0, label_indices]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "006d03483257a8d0c9391e38bcb8afa5df89814763ae9a97f4cfe164c8959f65"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
