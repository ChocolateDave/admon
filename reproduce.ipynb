{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display\n",
    "from typing import Union\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "from admon.model import DMoN\n",
    "from admon.utils import load_npz, normalize_graph, modularity\n",
    "\n",
    "# Alias\n",
    "_PathLike = Union[str, 'os.PathLike[str]']\n",
    "CORA_DIR: _PathLike = './data/cora'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features, labels, label_indices = load_npz(os.path.join(CORA_DIR, 'cora.npz'))\n",
    "adj_tensor = T.tensor(adj.todense()).unsqueeze(0).float()\n",
    "features_tensor = T.tensor(features.todense()).unsqueeze(0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "DMoN                                     --\n",
       "├─Single: 1-1                            --\n",
       "│    └─Sequential: 2-1                   --\n",
       "│    │    └─GCN: 3-1                     91,712\n",
       "│    └─Linear: 2-2                       1,040\n",
       "│    └─Dropout: 2-3                      --\n",
       "=================================================================\n",
       "Total params: 92,752\n",
       "Trainable params: 92,752\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameter\n",
    "n_clusters: int = 16\n",
    "num_epochs: int = 200\n",
    "hidden: int = 64\n",
    "depths: int = 1\n",
    "dropout: float = 0.3\n",
    "inflation: int = 1\n",
    "collapse_regularization = 1\n",
    "device = T.device('cpu')\n",
    "\n",
    "lr: float = 1e-3\n",
    "weight_decay: float = 5e-4\n",
    "lr_decay_step: int = 5\n",
    "lr_decay_gamma: float = 0.3\n",
    "\n",
    "model = DMoN(features_tensor.size(-1), n_clusters,\n",
    "             hidden, depths, dropout, inflation,\n",
    "             collapse_regularization=collapse_regularization)\n",
    "model: nn.Module = model.to(device)\n",
    "torchinfo.summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] Loss: 2707.08\n",
      "Epoch [2/200] Loss: 2707.05\n",
      "Epoch [3/200] Loss: 2707.06\n",
      "Epoch [4/200] Loss: 2707.03\n",
      "Epoch [5/200] Loss: 2707.00\n",
      "Epoch [6/200] Loss: 2707.00\n",
      "Epoch [7/200] Loss: 2707.01\n",
      "Epoch [8/200] Loss: 2707.01\n",
      "Epoch [9/200] Loss: 2707.00\n",
      "Epoch [10/200] Loss: 2707.00\n",
      "Epoch [11/200] Loss: 2706.99\n",
      "Epoch [12/200] Loss: 2706.99\n",
      "Epoch [13/200] Loss: 2706.99\n",
      "Epoch [14/200] Loss: 2706.99\n",
      "Epoch [15/200] Loss: 2706.99\n",
      "Epoch [16/200] Loss: 2706.99\n",
      "Epoch [17/200] Loss: 2706.99\n",
      "Epoch [18/200] Loss: 2706.98\n",
      "Epoch [19/200] Loss: 2706.98\n",
      "Epoch [20/200] Loss: 2706.98\n",
      "Epoch [21/200] Loss: 2706.98\n",
      "Epoch [22/200] Loss: 2706.98\n",
      "Epoch [23/200] Loss: 2706.98\n",
      "Epoch [24/200] Loss: 2706.98\n",
      "Epoch [25/200] Loss: 2706.98\n",
      "Epoch [26/200] Loss: 2706.98\n",
      "Epoch [27/200] Loss: 2706.98\n",
      "Epoch [28/200] Loss: 2706.98\n",
      "Epoch [29/200] Loss: 2706.98\n",
      "Epoch [30/200] Loss: 2706.98\n",
      "Epoch [31/200] Loss: 2706.98\n",
      "Epoch [32/200] Loss: 2706.98\n",
      "Epoch [33/200] Loss: 2706.98\n",
      "Epoch [34/200] Loss: 2706.98\n",
      "Epoch [35/200] Loss: 2706.98\n",
      "Epoch [36/200] Loss: 2706.98\n",
      "Epoch [37/200] Loss: 2706.98\n",
      "Epoch [38/200] Loss: 2706.98\n",
      "Epoch [39/200] Loss: 2706.98\n",
      "Epoch [40/200] Loss: 2706.98\n",
      "Epoch [41/200] Loss: 2706.98\n",
      "Epoch [42/200] Loss: 2706.98\n",
      "Epoch [43/200] Loss: 2706.98\n",
      "Epoch [44/200] Loss: 2706.98\n",
      "Epoch [45/200] Loss: 2706.98\n",
      "Epoch [46/200] Loss: 2706.98\n",
      "Epoch [47/200] Loss: 2706.98\n",
      "Epoch [48/200] Loss: 2706.98\n",
      "Epoch [49/200] Loss: 2706.98\n",
      "Epoch [50/200] Loss: 2706.98\n",
      "Epoch [51/200] Loss: 2706.98\n",
      "Epoch [52/200] Loss: 2706.98\n",
      "Epoch [53/200] Loss: 2706.98\n",
      "Epoch [54/200] Loss: 2706.98\n",
      "Epoch [55/200] Loss: 2706.98\n",
      "Epoch [56/200] Loss: 2706.98\n",
      "Epoch [57/200] Loss: 2706.98\n",
      "Epoch [58/200] Loss: 2706.98\n",
      "Epoch [59/200] Loss: 2706.98\n",
      "Epoch [60/200] Loss: 2706.98\n",
      "Epoch [61/200] Loss: 2706.98\n"
     ]
    }
   ],
   "source": [
    "# Train main\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, lr_decay_step, lr_decay_gamma)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  pooled_features, assignments = model((features_tensor, adj_tensor))\n",
    "  model.loss.backward()\n",
    "  optimizer.step()\n",
    "  lr_scheduler.step()\n",
    "\n",
    "  print(f'Epoch [{epoch+1:d}/{num_epochs:d}] Loss: {model.loss.item():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 9, 7, ..., 9, 9, 9]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assignments.detach().cpu().numpy().argmax(axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.sparse' has no attribute 'spmm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/juanwu/Documents/spring_22/CE290_2/project/modularity_cluster_aviation/test.ipynb Cell 6'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/juanwu/Documents/spring_22/CE290_2/project/modularity_cluster_aviation/test.ipynb#ch0000006vscode-remote?line=0'>1</a>\u001b[0m T\u001b[39m.\u001b[39;49msparse\u001b[39m.\u001b[39;49mspmm(adj_tensor, features_tensor)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.sparse' has no attribute 'spmm'"
     ]
    }
   ],
   "source": [
    "T.sparse.spmm(adj_tensor, features_tensor)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "006d03483257a8d0c9391e38bcb8afa5df89814763ae9a97f4cfe164c8959f65"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
